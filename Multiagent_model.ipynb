{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:204: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "from Model.DL_Network_Model import Net\n",
    "from Model.Funtion_Bank import data_input\n",
    "import os.path\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "from PIL import *\n",
    "import PIL.Image\n",
    "import random\n",
    "\n",
    "input_file_path = 'Data/training_data_input.csv'\n",
    "score_file_path = 'Data/training_data_score.csv'\n",
    "next_action_file_path = 'Data/training_data_next_action_taken.csv'\n",
    "#input_file_path_initiate = 'Data/training_data_input_initiate.csv'\n",
    "#score_file_path_initiate = 'Data/training_data_score_initiate.csv'\n",
    "training_file_path = 'Data/training_data.csv'\n",
    "\n",
    "DQ_ratio = 0.9\n",
    "file_name_model_latest_version = 'Model/model_latest_version.pt'\n",
    "file_name_model_last_version = 'Model/model_last_version.pt'\n",
    "\n",
    "#data format setup:\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "input_data = torch.tensor([0,0,0,0,0,0,0,0,0], dtype=torch.float)\n",
    "\n",
    "def play_ground(h, w, agent_a, agent_b, agent_c, agent_d, ball):\n",
    "    rows = h\n",
    "    columns = w * 3 + 2\n",
    "    init_ground = torch.tensor([[0 for x in range(columns)] for y in range(rows)], dtype=torch.float)\n",
    "    for i in range(0, rows):\n",
    "        for j in [w, -w - 1]:\n",
    "            init_ground[i][j] = 1\n",
    "    for i in [agent_a[0]]:\n",
    "        for j in range(agent_a[1] + w, agent_a[3] + 1 + w):\n",
    "            init_ground[i][j] = 1\n",
    "    for i in [agent_b[0]]:\n",
    "        for j in range(agent_b[1] + w, agent_b[3] + 1 + w):\n",
    "            init_ground[i][j] = 1\n",
    "    for i in [agent_c[0]]:\n",
    "        for j in range(agent_c[3] + w, agent_c[1] + 1 + w):\n",
    "            init_ground[i][j] = 1\n",
    "    for i in [agent_d[0]]:\n",
    "        for j in range(agent_d[3] + w, agent_d[1] + 1 + w):\n",
    "            init_ground[i][j] = 1\n",
    "    init_ground[ball[0]][ball[1]] = 2\n",
    "    return init_ground\n",
    "\n",
    "def agent_ball(perception, state):\n",
    "    rows = 8\n",
    "    columns = 3\n",
    "    reference_matrix = torch.tensor([[0 for x in range(columns)] for y in range(rows)], dtype=torch.int)\n",
    "    reference_matrix[0][1] = -1\n",
    "    reference_matrix[0][2] = -1\n",
    "    reference_matrix[1][1] = -1\n",
    "    reference_matrix[1][2] = 0\n",
    "    reference_matrix[2][1] = -1\n",
    "    reference_matrix[2][2] = 1\n",
    "    reference_matrix[3][1] = 0\n",
    "    reference_matrix[3][2] = 1\n",
    "    reference_matrix[4][1] = 1\n",
    "    reference_matrix[4][2] = 1\n",
    "    reference_matrix[5][1] = 1\n",
    "    reference_matrix[5][2] = 0\n",
    "    reference_matrix[6][1] = 1\n",
    "    reference_matrix[6][2] = -1\n",
    "    reference_matrix[7][1] = 0\n",
    "    reference_matrix[7][2] = -1\n",
    "\n",
    "    if (state[2] * state[3]) == 0:\n",
    "        if perception == [1, 1, 1]:\n",
    "            new_speed_x = -state[2]\n",
    "            new_speed_y = -state[3]\n",
    "        elif perception == [0, 1, 1]:\n",
    "            for i in range(0,8):\n",
    "                if (state[2] == reference_matrix[i][1]) & (state[3] == reference_matrix[i][2]):\n",
    "                    start_point = i\n",
    "                    print(perception)\n",
    "                    print(start_point)\n",
    "            new_speed_x = reference_matrix[start_point - 2][1]\n",
    "            new_speed_y = reference_matrix[start_point - 2][2]\n",
    "        elif perception == [1, 1, 0]:\n",
    "            for i in range(0,8):\n",
    "                if (state[2] == reference_matrix[i][1]) & (state[3] == reference_matrix[i][2]):\n",
    "                    start_point = i\n",
    "            new_speed_x = reference_matrix[(start_point + 2)% 8][1]\n",
    "            new_speed_y = reference_matrix[(start_point + 2)% 8][2]\n",
    "        elif perception == [0, 1, 0]:\n",
    "            new_speed_x = -state[2]\n",
    "            new_speed_y = -state[3]\n",
    "        else:\n",
    "            new_speed_x = state[2]\n",
    "            new_speed_y = state[3]\n",
    "    else:\n",
    "        if (perception[0] == 1) & (perception[2] == 1):\n",
    "            new_speed_x = -state[2]\n",
    "            new_speed_y = -state[3]\n",
    "        elif perception == [0, 1, 0]:\n",
    "            new_speed_x = -state[2]\n",
    "            new_speed_y = -state[3]\n",
    "        elif perception[2] == 1:\n",
    "            for i in range(0,8):\n",
    "                if (state[2] == reference_matrix[i][1]) & (state[3] == reference_matrix[i][2]):\n",
    "                    start_point = i\n",
    "                    print(perception)\n",
    "                    print(start_point)\n",
    "            new_speed_x = reference_matrix[start_point - 2][1]\n",
    "            new_speed_y = reference_matrix[start_point - 2][2]\n",
    "        elif perception[0] == 1:\n",
    "            for i in range(0,8):\n",
    "                if (state[2] == reference_matrix[i][1]) & (state[3] == reference_matrix[i][2]):\n",
    "                    start_point = i\n",
    "            new_speed_x = reference_matrix[(start_point + 2)% 8][1]\n",
    "            new_speed_y = reference_matrix[(start_point + 2)% 8][2]\n",
    "        else:\n",
    "            new_speed_x = state[2]\n",
    "            new_speed_y = state[3]\n",
    "\n",
    "    action = [new_speed_x, new_speed_y]\n",
    "    state = [state[0] + new_speed_x, state[1] + new_speed_y, new_speed_x, new_speed_y]\n",
    "    return (action, state)\n",
    "\n",
    "\n",
    "def func_perspection(play_ground, state):\n",
    "    rows = 8\n",
    "    columns = 3\n",
    "    reference_matrix = torch.tensor([[0 for x in range(columns)] for y in range(rows)], dtype=torch.int)\n",
    "    reference_matrix[0][1] = -1\n",
    "    reference_matrix[0][2] = -1\n",
    "    reference_matrix[1][1] = -1\n",
    "    reference_matrix[1][2] = 0\n",
    "    reference_matrix[2][1] = -1\n",
    "    reference_matrix[2][2] = 1\n",
    "    reference_matrix[3][1] = 0\n",
    "    reference_matrix[3][2] = 1\n",
    "    reference_matrix[4][1] = 1\n",
    "    reference_matrix[4][2] = 1\n",
    "    reference_matrix[5][1] = 1\n",
    "    reference_matrix[5][2] = 0\n",
    "    reference_matrix[6][1] = 1\n",
    "    reference_matrix[6][2] = -1\n",
    "    reference_matrix[7][1] = 0\n",
    "    reference_matrix[7][2] = -1\n",
    "    for i in range(0,8):\n",
    "        reference_matrix[i][0] = play_ground[state[0] + reference_matrix[i][1]][state[1] + reference_matrix[i][2]]\n",
    "        if (state[2] == reference_matrix[i][1]) & (state[3] == reference_matrix[i][2]):\n",
    "            start_point = i\n",
    "    #print(reference_matrix)\n",
    "    if (state[2] * state[3]) == 0:\n",
    "        #print(state[2])\n",
    "        #print(state[3])\n",
    "        new_perception = [reference_matrix[start_point - 2][0], reference_matrix[start_point][0], reference_matrix[(start_point + 2)% 8][0]]\n",
    "    else:\n",
    "        new_perception = [reference_matrix[start_point - 1][0], reference_matrix[start_point][0], reference_matrix[(start_point + 1)% 8][0]]\n",
    "\n",
    "    #print(new_perception)\n",
    "    return new_perception\n",
    "\n",
    "\n",
    "def DL_training_model():\n",
    "    indicator = 0\n",
    "    learning_rate = 0.1\n",
    "    epoch_size = 10000\n",
    "    steps_for_printing_out_loss = 1000\n",
    "    #cropped_perspective = np.array([4,5,6])\n",
    "    input_data = data_input(input_file_path)\n",
    "    score_data = data_input(score_file_path)\n",
    "    input = input_data\n",
    "    #print(input)\n",
    "    target = score_data\n",
    "    net = Net()\n",
    "    loss_functioin = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr = learning_rate)\n",
    "    if indicator == 1:\n",
    "        state_dict_last_version = torch.load('Model/model_latest_version.pt')['state_dict']\n",
    "        net.load_state_dict(state_dict_last_version)\n",
    "    for i in range(1,epoch_size + 1):\n",
    "        optimizer.zero_grad()\n",
    "        output = net(input)\n",
    "        output[target == -2] = -2\n",
    "        #output[target == -1.1] = -1.1\n",
    "        loss = loss_functioin(output, target)\n",
    "        loss.backward()\n",
    "        if i % (steps_for_printing_out_loss) == 0:\n",
    "            print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\n",
    "        # Does the update\n",
    "        optimizer.step()\n",
    "    \n",
    "    if indicator == 1:\n",
    "        copyfile(file_name_model_latest_version, file_name_model_last_version)\n",
    "    else:\n",
    "        torch.save({'state_dict': net.state_dict(),'optimizer': optimizer.state_dict()}, file_name_model_last_version)\n",
    "    torch.save({'state_dict': net.state_dict(),'optimizer': optimizer.state_dict()}, file_name_model_latest_version)\n",
    "\n",
    "\n",
    "def DL_model(cropped_perspective, model_version, state):\n",
    "    model_latest_version = Net()\n",
    "    state_dict_latest_version = torch.load(file_name_model_latest_version)['state_dict']\n",
    "    model_latest_version.load_state_dict(state_dict_latest_version)\n",
    "    next_vision = model_latest_version(torch.tensor(cropped_perspective.reshape(252), dtype=torch.float))\n",
    "    #print(next_vision)\n",
    "    if ((state[0] == -1) and (state[1] == 1)) or ((state[0] == 0) and (state[1] == playground_w)):\n",
    "        next_vision[0] = -3\n",
    "    if ((state[0] == -1) and (state[3] == playground_w)) or ((state[0] == 0) and (state[3] == 1)):\n",
    "        next_vision[2] = -3\n",
    "    #print('abc')\n",
    "    #print(next_vision)\n",
    "    next_step = next_vision.argmax()\n",
    "    action = next_step\n",
    "    return action\n",
    "\n",
    "def game_agent(play_ground_matrix, state):\n",
    "    result = 0\n",
    "    if (2 in play_ground_matrix[0]):\n",
    "        result = 'a'\n",
    "    if (2 in play_ground_matrix[-1]):\n",
    "        result = 'b'\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def cropping_perspective(perception, state):\n",
    "    mid_point = (state[1] + state[3]) / 2 + playground_w + 1\n",
    "    if state[0] == -1:\n",
    "        cropped_perspective = perception[:,int(mid_point - playground_w): int(mid_point + playground_w + 1)]\n",
    "        cropped_perspective = cropped_perspective.cpu().numpy()\n",
    "    else:\n",
    "        cropped_perspective = perception[:,int(mid_point - playground_w): int(mid_point + playground_w + 1)]\n",
    "        #print(int(mid_point - playground_w))\n",
    "        #print(int(mid_point + playground_w))\n",
    "        #cropped_perspective = perception[:][2:4]\n",
    "        cropped_perspectiven_numpy = cropped_perspective.clone()\n",
    "        cropped_perspective = np.flip(cropped_perspectiven_numpy.cpu().numpy(), 0)\n",
    "        #print(cropped_perspective)\n",
    "        #cropped_perspective = torch.tensor(cropped_perspective, dtype=torch.float)\n",
    "    #print(cropped_perspective)\n",
    "    cropped_perspective = torch.tensor(cropped_perspective.reshape(252), dtype=torch.float)\n",
    "    return cropped_perspective\n",
    "    \n",
    "def agent_a(cropped_perspective, state):\n",
    "    action = DL_model(cropped_perspective, 1, state) #-1, 1, 0\n",
    "    #state = [-1, 1, -1, 3]\n",
    "    if state[0] == 0:\n",
    "        state[1] = state[1] - (action - 1)\n",
    "        state[3] = state[3] - (action - 1)\n",
    "    else:\n",
    "        state[1] = state[1] + (action - 1)\n",
    "        state[3] = state[3] + (action - 1)\n",
    "    return (action, state)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    k = 0\n",
    "    if k == 0:\n",
    "        playground_h = 12\n",
    "        playground_w = 10\n",
    "        #print(play_ground(12,10,[-1, 1, -1, 3],[-1, 4, -1, 6], [0, 6, 0, 4], [0, 3, 0, 1], [3, 3]))\n",
    "        #New_play_ground = play_ground(12,10,[-1, 1, -1, 3],[-1, 4, -1, 6], [0, 6, 0, 4], [0, 3, 0, 1], [3, 3])\n",
    "        #print(agent_ball([1, 0, 0], [5, 15, 1, 1]))\n",
    "        for rounds in range(0, 1):\n",
    "\n",
    "            current_state = [9, 16, 1, 1]\n",
    "            agent_a1_state = [-1, 1, -1, 3]\n",
    "            agent_a2_state = [-1, 4, -1, 6]\n",
    "            agent_b1_state = [0, 6, 0, 4]\n",
    "            agent_b2_state = [0, 3, 0, 1]\n",
    "\n",
    "            random_a1 = random.randint(1,6)\n",
    "            random_a2 = random.randint(1,6)\n",
    "            random_b1 = random.randint(1,6)\n",
    "            random_b2 = random.randint(1,6)\n",
    "\n",
    "            current_state = [random.randint(3,9), random.randint(12,19), random.choice([1, 0, -1]), random.choice([1, 0, -1])]\n",
    "            if (current_state[2] * current_state[3]) == 0:\n",
    "                current_state[2] = 1\n",
    "                current_state[3] = -1\n",
    "            agent_a1_state = [-1, 1 + random_a1, -1, 3 + random_a1]\n",
    "            agent_a2_state = [-1, 1 + random_a2, -1, 3 + random_a2]\n",
    "            agent_b1_state = [0, 10 - random_b1, 0, 8 - random_b1]\n",
    "            agent_b2_state = [0, 10 - random_b2, 0, 8 - random_b2]\n",
    "\n",
    "            #New_action, New_state = agent_ball([0, 0, 0], [5, 16, 1, 1])\n",
    "\n",
    "            all_cropped_perspective_agent_a1 = []\n",
    "            all_action_agent_a1 = []\n",
    "            all_score_agent_a1 = []\n",
    "\n",
    "            all_cropped_perspective_agent_a2 = []\n",
    "            all_action_agent_a2 = []\n",
    "            all_score_agent_a2 = []\n",
    "\n",
    "            all_cropped_perspective_agent_b1 = []\n",
    "            all_action_agent_b1 = []\n",
    "            all_score_agent_b1 = []\n",
    "\n",
    "            all_cropped_perspective_agent_b2 = []\n",
    "            all_action_agent_b2 = []\n",
    "            all_score_agent_b2 = []\n",
    "\n",
    "            score_agent_a1 = 0\n",
    "            score_agent_a2 = 0\n",
    "            score_agent_b1 = 0\n",
    "            score_agent_b2 = 0\n",
    "\n",
    "            for i in range(0,30):\n",
    "                Current_play_ground = play_ground(12,10,agent_a1_state, agent_a2_state, agent_b1_state, agent_b2_state, current_state[0: 2])\n",
    "                #print(Current_play_ground)\n",
    "                #print(game_agent(Current_play_ground, [1]))\n",
    "                Current_perception = func_perspection(Current_play_ground, current_state)\n",
    "\n",
    "                New_action, New_state = agent_ball(Current_perception, current_state)\n",
    "\n",
    "                Current_play_ground[current_state[0]][current_state[1]] = -2\n",
    "                current_state = New_state\n",
    "                Current_play_ground[current_state[0]][current_state[1]] = 2\n",
    "\n",
    "                #print(agent_b2_state)\n",
    "                cropped_perspective_agent_a1 = cropping_perspective(Current_play_ground, agent_a1_state)\n",
    "                cropped_perspective_agent_a2 = cropping_perspective(Current_play_ground, agent_a2_state)\n",
    "                cropped_perspective_agent_b1 = cropping_perspective(Current_play_ground, agent_b1_state)\n",
    "                cropped_perspective_agent_b2 = cropping_perspective(Current_play_ground, agent_b2_state)\n",
    "\n",
    "\n",
    "                agent_a1_action, agent_a1_state = agent_a(cropped_perspective_agent_a1, agent_a1_state)\n",
    "                agent_a2_action, agent_a2_state = agent_a(cropped_perspective_agent_a2, agent_a2_state)\n",
    "                agent_b1_action, agent_b1_state = agent_a(cropped_perspective_agent_b1, agent_b1_state)\n",
    "                agent_b2_action, agent_b2_state = agent_a(cropped_perspective_agent_b2, agent_b2_state)\n",
    "\n",
    "                current_score = torch.ones(3) * -2\n",
    "\n",
    "                all_cropped_perspective_agent_a1.append(cropped_perspective_agent_a1.cpu().numpy())\n",
    "                all_action_agent_a1.append(agent_a1_action.cpu().numpy())\n",
    "                all_score_agent_a1.append(current_score.cpu().numpy()) \n",
    "\n",
    "                all_cropped_perspective_agent_a2.append(cropped_perspective_agent_a2.cpu().numpy())\n",
    "                all_action_agent_a2.append(agent_a2_action.cpu().numpy())\n",
    "                all_score_agent_a2.append(current_score.cpu().numpy()) \n",
    "\n",
    "                all_cropped_perspective_agent_b1.append(cropped_perspective_agent_b1.cpu().numpy())\n",
    "                all_action_agent_b1.append(agent_b1_action.cpu().numpy())\n",
    "                all_score_agent_b1.append(current_score.cpu().numpy()) \n",
    "\n",
    "                all_cropped_perspective_agent_b2.append(cropped_perspective_agent_b2.cpu().numpy())\n",
    "                all_action_agent_b2.append(agent_b2_action.cpu().numpy())\n",
    "                all_score_agent_b2.append(current_score.cpu().numpy()) \n",
    "\n",
    "\n",
    "                if game_agent(Current_play_ground, [1]) == 'a':\n",
    "                    score_agent_a1 = 1\n",
    "                    score_agent_a2 = 1\n",
    "                    score_agent_b1 = -1\n",
    "                    score_agent_b2 = -1\n",
    "                    break\n",
    "                elif game_agent(Current_play_ground, [1]) == 'b':\n",
    "                    score_agent_a1 = -1\n",
    "                    score_agent_a2 = -1\n",
    "                    score_agent_b1 = 1\n",
    "                    score_agent_b2 = 1\n",
    "                    break\n",
    "\n",
    "                #print(all_cropped_perspective_agent_a1)\n",
    "                #print(all_action_agent_a1)\n",
    "                Current_play_ground = play_ground(12,10,agent_a1_state, agent_a2_state, agent_b1_state, agent_b2_state, current_state[0: 2])\n",
    "\n",
    "                export_output = Current_play_ground.clone()\n",
    "                export_output = export_output.cpu().detach().numpy()\n",
    "                export_output = export_output * 120\n",
    "                img = Image.fromarray(export_output)\n",
    "                #img.save('my.png')\n",
    "                img.show()\n",
    "\n",
    "                #print(New_action)\n",
    "                #print(New_state)\n",
    "                #print(new_perception)\n",
    "            #print(agent_a(Current_play_ground, [-1, 1, -1, 3]))\n",
    "            for k in range(0, len(all_action_agent_a1)):\n",
    "                all_score_agent_a1[-(k+1)][all_action_agent_a1[-(k+1)]] = ((DQ_ratio)**(k)) * score_agent_a1\n",
    "                all_score_agent_a2[-(k+1)][all_action_agent_a2[-(k+1)]] = ((DQ_ratio)**(k)) * score_agent_a2\n",
    "                all_score_agent_b1[-(k+1)][all_action_agent_b1[-(k+1)]] = ((DQ_ratio)**(k)) * score_agent_b1\n",
    "                all_score_agent_b2[-(k+1)][all_action_agent_b2[-(k+1)]] = ((DQ_ratio)**(k)) * score_agent_b2\n",
    "            #print(all_score_agent_a1)\n",
    "            #print(all_score_agent_a2)\n",
    "            #print(all_score_agent_b1)\n",
    "            #print(all_score_agent_b2)\n",
    "\n",
    "            input_status = all_cropped_perspective_agent_a1 + all_cropped_perspective_agent_a2 + all_cropped_perspective_agent_b1 + all_cropped_perspective_agent_b2\n",
    "            next_action_taken = all_action_agent_a1 + all_action_agent_a2 + all_action_agent_b1 + all_action_agent_b2\n",
    "            score = all_score_agent_a1 + all_score_agent_a2 + all_score_agent_b1 + all_score_agent_b2\n",
    "\n",
    "            input_status_df = pd.DataFrame(input_status)\n",
    "            input_status_df.to_csv(input_file_path, index=False, mode='a', header=False)\n",
    "            next_action_taken_df = pd.DataFrame(next_action_taken)\n",
    "            next_action_taken_df.to_csv(next_action_file_path, index=False, mode='a', header=False)\n",
    "            score_df = pd.DataFrame(score)\n",
    "            score_df.to_csv(score_file_path, index=False, mode='a', header=False)\n",
    "\n",
    "        \"\"\"\n",
    "        #remove duplication:\n",
    "        input_status_df = pd.read_csv(input_file_path, header = None)\n",
    "        next_action_taken_df = pd.read_csv(next_action_file_path, header = None)\n",
    "        score_df = pd.read_csv(score_file_path, header = None)\n",
    "        consul_df = pd.concat([input_status_df, next_action_taken_df, score_df], axis=1)\n",
    "        consul_df = consul_df.replace(-0.0, 0.0)\n",
    "        consul_df.drop_duplicates(keep = 'first', inplace = True) \n",
    "        input_status_df = consul_df.iloc[:,0:252].copy()\n",
    "        next_action_taken_df = consul_df.iloc[:,252:253].copy()\n",
    "        score_df = consul_df.iloc[:,253:].copy()\n",
    "        input_status_df.to_csv(input_file_path, index=False, header=False)\n",
    "        next_action_taken_df.to_csv(next_action_file_path, index=False, header=False)\n",
    "        score_df.to_csv(score_file_path, index=False, header=False)\n",
    "        \"\"\"\n",
    "    else:\n",
    "        DL_training_model()\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "#print(New_play_ground)\n",
    "export_output = Current_play_ground.clone()\n",
    "export_output = export_output.cpu().detach().numpy()\n",
    "export_output = export_output * 120\n",
    "img = Image.fromarray(export_output)\n",
    "#img.save('my.png')\n",
    "img.show()\n",
    "#print(func_perspection(New_play_ground, New_state))\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0   1   2   3   4   5   6   7   8   9]\n",
      "  [ 10  11  12  13  14  15  16  17  18  19]\n",
      "  [ 20  21  22  23  24  25  26  27  28  29]\n",
      "  [ 30  31  32  33  34  35  36  37  38  39]\n",
      "  [ 40  41  42  43  44  45  46  47  48  49]]\n",
      "\n",
      " [[ 50  51  52  53  54  55  56  57  58  59]\n",
      "  [ 60  61  62  63  64  65  66  67  68  69]\n",
      "  [ 70  71  72  73  74  75  76  77  78  79]\n",
      "  [ 80  81  82  83  84  85  86  87  88  89]\n",
      "  [ 90  91  92  93  94  95  96  97  98  99]]\n",
      "\n",
      " [[100 101 102 103 104 105 106 107 108 109]\n",
      "  [110 111 112 113 114 115 116 117 118 119]\n",
      "  [120 121 122 123 124 125 126 127 128 129]\n",
      "  [130 131 132 133 134 135 136 137 138 139]\n",
      "  [140 141 142 143 144 145 146 147 148 149]]\n",
      "\n",
      " [[150 151 152 153 154 155 156 157 158 159]\n",
      "  [160 161 162 163 164 165 166 167 168 169]\n",
      "  [170 171 172 173 174 175 176 177 178 179]\n",
      "  [180 181 182 183 184 185 186 187 188 189]\n",
      "  [190 191 192 193 194 195 196 197 198 199]]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "crop Matrix for 4 Agents\n",
    "action: 1,2,3, array\n",
    "RL model\n",
    "\n",
    "\n",
    "draft: 3 output: score: 1, 0, -1???\n",
    "draft: DL model\n",
    "\n",
    "completed: save and load 3D data into local file (mat file)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "# Some test data\n",
    "x = np.arange(200).reshape((4,5,10))\n",
    "\n",
    "# Specify the filename of the .mat file\n",
    "matfile = 'test_mat.mat'\n",
    "\n",
    "# Write the array to the mat file. For this to work, the array must be the value\n",
    "# corresponding to a key name of your choice in a dictionary\n",
    "scipy.io.savemat(matfile, mdict={'out': x}, oned_as='row')\n",
    "\n",
    "# For the above line, I specified the kwarg oned_as since python (2.7 with \n",
    "# numpy 1.6.1) throws a FutureWarning.  Here, this isn't really necessary \n",
    "# since oned_as is a kwarg for dealing with 1-D arrays.\n",
    "\n",
    "# Now load in the data from the .mat that was just saved\n",
    "matdata = scipy.io.loadmat(matfile)\n",
    "\n",
    "# And just to check if the data is the same:\n",
    "assert np.all(x == matdata['out'])\n",
    "\n",
    "print(matdata['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [1,2,3]\n",
    "b = a[1: 1]\n",
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
